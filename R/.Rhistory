#We put as stopwords common english words as well as the names of the brands and products
my_stop_words <-
c(
word = c(
"#",
"s",
"ve",
"re",
"skin",
"sunscreen",
"product",
"spf",
brands,
productnames,
"shield",
"sunscreen",
"sunscreens",
"t",
"it",
"It",
"use"
),
c(stopwords::stopwords("en"))
) %>% as_tibble()
my_stop_words$word <- my_stop_words$value
#We create a review2 column that remains untouched and we tokenize review
#We then take away the stop words
sunscreen_cleaned <- sunscreen %>%
mutate(review2 = review) %>%
as.tibble() %>%
unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
anti_join(my_stop_words, by = "word") %>%
filter(is.na(as.numeric(word)))
#tf-idf: we want to see if some words look specific to a review or to a product.
tf_byreview <- sunscreen_cleaned %>%
group_by(reviewId) %>%
count(word) %>%
ungroup()
tfidf_byreview <- tf_byreview %>%
tidytext::bind_tf_idf(word, reviewId, n)
tfidf_byreview %>%
dplyr::arrange(desc(tf_idf)) %>%
head() %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
)
tf_byproduct <- sunscreen_cleaned %>%
group_by(productName) %>%
count(word) %>%
ungroup()
tfidf_product <- tf_byproduct %>%
bind_tf_idf(word, productName, n)
tfidf_product %>%
dplyr::arrange(desc(tf_idf)) %>%
head() %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
)
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
sunscreen$sentiments <- sunscreen$sentiments %>%
as.numeric()
sentiment_valence_shifter_bybrand <- sunscreen %>%
group_by(brandName) %>%
mutate(sentimentbybrand = mean(sentiment))
sentiment_valence_shifter_byproduct <-
sentiment_valence_shifter_bybrand %>%
group_by(productName) %>%
mutate(sentimentbyproduct = mean(sentiment))
par(mfrow = c(3, 3))
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))
}
# LDA
## convert quateda object to topicmodels object
dtm <- convert(dfmat, to = "topicmodels")
##SIMILARITIES:
sunscreen_nostopword_corpus <- sunscreen %>%
mutate(review2 = review) %>%
as.tibble()
sunscreen_corpus <- corpus(sunscreen_nostopword_corpus$review) %>%
quanteda::tokens(
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_separators = TRUE,
remove_twitter = TRUE,
remove_hyphens = TRUE,
remove_url = TRUE,
ngrams = 1L,
skip = 0L,
concatenator = "_",
verbose = quanteda_options("verbose"),
include_docvars = TRUE
) %>%
tokens_select(pattern = stopwords('en'), selection = 'remove')
dfmat <- dfm(sunscreen_corpus,
remove_punct = TRUE,
remove = my_stop_words$word)
#method = cosine
(tstat2 <-
textstat_simil(dfmat, method = "cosine", margin = "documents"))
sum(dfmat[1, ] * dfmat[2, ]) / sqrt(sum(dfmat[1, ] ^ 2) * sum(dfmat[2, ] ^
2))
par(mfrow = c(1, 1))
plot(hclust(as.dist(1 - tstat2)))
# LDA
## convert quateda object to topicmodels object
dtm <- convert(dfmat, to = "topicmodels")
lda <- LDA(dtm, k = 10) # build 10 topics
terms(lda, 5) # see the 5 terms most associated with each topic
topics(lda, 5) # see the 5 topics most associated with each documents
## Extract the beta and gamma from topicmodels object
lda@beta[, 1:10] # 1 first words
lda@gamma
# Save beta and gamma results and do matrix multiplication between the 2 matrices for supervised learning
beta <- lda@beta[, 1:10]
gam <- lda@gamma
mlda <- gam %*% beta
## show the betas of each document
beta.td <- tidy(lda, matrix = "beta")
beta.td
filter(beta.td, topic == 1) ## all for topic 1
## describes the topics with their most associated terms
beta.top.terms <- beta.td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic,-beta)
beta.top.terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap( ~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_minimal() +
theme(text = element_text(size = 8),
axis.text.x = element_text(angle = 90))
## describes the topics in each documents
gamma.td <- tidy(lda, matrix = "gamma")
gettabletopicLDA <- function(gamma, i) {
topic <- gamma %>% filter(topic == i)
topic <- topic[order(-topic$gamma), ] %>% head()
}
for (i in c(1:10)) {
gettabletopicLDA(gamma.td, i) %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
) %>% print()
}
## Assignment of topic to term in each document
augment(lda) %>% head() %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
) %>% print()
plot_sentiment_pagenormalized_sun <- function(p) {
ggplot(sentiment_normalized_perbrand_sun,
aes(
x = reorder(brandName,-n),
y = norm,
fill = sentiment
)) +
geom_bar(stat = "identity") +
ggforce::facet_wrap_paginate(
facets = ~ sentiment,
nrow = 3,
ncol = 4,
page = p
) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "", y = "Number of words", fill = "Sentiment")
}
plot_sentiment_pagenormalized_sun(1) %>% plot()
###FIRST ANALYSIS => PUT SPF out
##########Sentiments:
get_sunsentiments <- function(lexicon = c("sunscReen")) {
lexicon <- match.arg(lexicon)
sunscReen = lexicon_sunscReen()
}
lexicon_sunscReen <- function() {
readRDS("../data/sunscReen.rds")
}
spf <- str_extract_all(sunscreen$productName, "\\d+") %>%
as.data.frame() %>%
t()
sunscreen <- cbind(sunscreen, spf)
par(mfrow = c(1, 1))
#Try per brand
my_stop_words <- as_tibble(my_stop_words)
my_stop_words$word <- my_stop_words$value
sunscreen_cleaned_for_wordcloud <- sunscreen %>%
dplyr::mutate(review2 = review) %>%
dplyr::group_by(brandName) %>%
as.tibble() %>%
tidytext::unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
anti_join(my_stop_words, by = "word") %>%
dplyr::filter(is.na(as.numeric(word)))
#Sentiment analysis by brand
#sentiment with nrc: these are by products
sentiment_by_brand_sun <- sunscreen_cleaned_for_wordcloud %>%
inner_join(get_sunsentiments("sunscReen"), by = "word") %>%
group_by(brandName, sentiment) %>%
count()
sentiment_normalized_perbrand_sun <- sentiment_by_brand_sun %>%
group_by(brandName) %>%
mutate(norm = n / sum(n))
plot_sentiment_pagenormalized_sun <- function(p) {
ggplot(sentiment_normalized_perbrand_sun,
aes(
x = reorder(brandName,-n),
y = norm,
fill = sentiment
)) +
geom_bar(stat = "identity") +
ggforce::facet_wrap_paginate(
facets = ~ sentiment,
nrow = 3,
ncol = 4,
page = p
) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "", y = "Number of words", fill = "Sentiment")
}
plot_sentiment_pagenormalized_sun(1) %>% plot()
###############################################################
#modify sentiments per product too with new
sunscreen_cleaned_for_wordcloud_product <- sunscreen %>%
dplyr::mutate(review2 = review) %>%
dplyr::group_by(productName) %>%
as.tibble() %>%
tidytext::unnest_tokens(word, review) %>%
anti_join(stop_words, by = "word") %>%
anti_join(my_stop_words, by = "word") %>%
dplyr::filter(is.na(as.numeric(word)))
sentiment_by_brand_sun_product <-
sunscreen_cleaned_for_wordcloud_product %>%
inner_join(sunscReen::get_sunsentiments("sunscReen"), by = "word") %>%
group_by(productName, sentiment) %>%
count()
sentiment_normalized_perbrand_sun_product <-
sentiment_by_brand_sun_product %>%
group_by(productName) %>%
mutate(norm = n / sum(n))
plot_sentiment_pagenormalized_sun_product <- function(p) {
ggplot(
sentiment_normalized_perbrand_sun_product,
aes(
x = reorder(productName,-n),
y = norm,
fill = sentiment
)
) +
geom_bar(stat = "identity") +
ggforce::facet_wrap_paginate(
facets = ~ sentiment,
nrow = 3,
ncol = 4,
page = p
) +
theme_bw() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "", y = "Number of words", fill = "Sentiment")
}
plot_sentiment_pagenormalized_sun_product(1) %>% plot()
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
sunscreen$sentiments <- sunscreen$sentiments %>%
as.numeric()
sentiment_valence_shifter_bybrand <- sunscreen %>%
group_by(brandName) %>%
mutate(sentimentbybrand = mean(sentiment))
sentiment_valence_shifter_byproduct <-
sentiment_valence_shifter_bybrand %>%
group_by(productName) %>%
mutate(sentimentbyproduct = mean(sentiment))
par(mfrow = c(3, 3))
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))
}
for (i in productnames) {
y <- sentiment_valence_shifter_byproduct %>%
filter(productName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))
}
for (i in productnames) {
y <- sentiment_valence_shifter_byproduct %>%
filter(productName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i), ylim=c(0,400))
}
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
sunscreen$sentiments <- sunscreen$sentiments %>%
as.numeric()
sentiment_valence_shifter_bybrand <- sunscreen %>%
group_by(brandName) %>%
mutate(sentimentbybrand = mean(sentiment))
sentiment_valence_shifter_byproduct <-
sentiment_valence_shifter_bybrand %>%
group_by(productName) %>%
mutate(sentimentbyproduct = mean(sentiment))
par(mfrow = c(3, 3))
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))
}
for (i in productnames) {
y <- sentiment_valence_shifter_byproduct %>%
filter(productName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i), ylim=c(0,400))
}
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
sunscreen$sentiments <- sunscreen$sentiments %>%
as.numeric()
sentiment_valence_shifter_bybrand <- sunscreen %>%
group_by(brandName) %>%
mutate(sentimentbybrand = mean(sentiment))
sentiment_valence_shifter_byproduct <-
sentiment_valence_shifter_bybrand %>%
group_by(productName) %>%
mutate(sentimentbyproduct = mean(sentiment))
par(mfrow = c(3, 3))
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i), ylim=c(0,400))
}
for (i in productnames) {
y <- sentiment_valence_shifter_byproduct %>%
filter(productName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i), ylim=c(0,400))
}
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))+coord_cartesian(ylim=c(0,400))
}
#Sentiments wuth valence shifters
for (i in c(1:length(sunscreen$review))) {
sunscreen$sentiments[i] <- sentiment_by(sunscreen$review[i],
hash_valence_shifters)
}
sunscreen$sentiments <- sunscreen$sentiments %>%
as.numeric()
sentiment_valence_shifter_bybrand <- sunscreen %>%
group_by(brandName) %>%
mutate(sentimentbybrand = mean(sentiment))
sentiment_valence_shifter_byproduct <-
sentiment_valence_shifter_bybrand %>%
group_by(productName) %>%
mutate(sentimentbyproduct = mean(sentiment))
par(mfrow = c(3, 3))
for (i in brands) {
x <- sentiment_valence_shifter_bybrand %>%
filter(brandName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))+coord_cartesian(ylim=c(0,400))
}
for (i in productnames) {
y <- sentiment_valence_shifter_byproduct %>%
filter(productName == i)
#summary(x$sentiment)%>%print()
boxplot(x$sentiments, main = paste(i))
}
##SIMILARITIES:
sunscreen_nostopword_corpus <- sunscreen %>%
mutate(review2 = review) %>%
as.tibble()
sunscreen_corpus <- corpus(sunscreen_nostopword_corpus$review) %>%
quanteda::tokens(
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_separators = TRUE,
remove_twitter = TRUE,
remove_hyphens = TRUE,
remove_url = TRUE,
ngrams = 1L,
skip = 0L,
concatenator = "_",
verbose = quanteda_options("verbose"),
include_docvars = TRUE
) %>%
tokens_select(pattern = stopwords('en'), selection = 'remove')
dfmat <- dfm(sunscreen_corpus,
remove_punct = TRUE,
remove = my_stop_words$word)
#method = cosine
(tstat2 <-
textstat_simil(dfmat, method = "cosine", margin = "documents"))
sum(dfmat[1, ] * dfmat[2, ]) / sqrt(sum(dfmat[1, ] ^ 2) * sum(dfmat[2, ] ^
2))
par(mfrow = c(1, 1))
plot(hclust(as.dist(1 - tstat2)))
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=5)
plot (clustering)
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=3)
plot (clustering)
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=3)
plot (clustering, xtat='n')
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=3)
plot (clustering.assignments)
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=3)
plot (cluster.assignments)
clustering <-hclust(as.dist(1 - tstat2))
cluster.assignments<-cutree(clustering, k=20)
plot (cluster.assignments)
plot (clustering)
# LDA
## convert quateda object to topicmodels object
dtm <- convert(dfmat, to = "topicmodels")
lda <- LDA(dtm, k = 10) # build 10 topics
terms(lda, 5) # see the 5 terms most associated with each topic
topics(lda, 5) # see the 5 topics most associated with each documents
## Extract the beta and gamma from topicmodels object
lda@beta[, 1:10] # 1 first words
lda@gamma
# Save beta and gamma results and do matrix multiplication between the 2 matrices for supervised learning
beta <- lda@beta[, 1:10]
gam <- lda@gamma
mlda <- gam %*% beta
## show the betas of each document
beta.td <- tidy(lda, matrix = "beta")
beta.td
filter(beta.td, topic == 1) ## all for topic 1
## describes the topics with their most associated terms
beta.top.terms <- beta.td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic,-beta)
beta.top.terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap( ~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_minimal() +
theme(text = element_text(size = 8),
axis.text.x = element_text(angle = 90))
## describes the topics in each documents
gamma.td <- tidy(lda, matrix = "gamma")
gettabletopicLDA <- function(gamma, i) {
topic <- gamma %>% filter(topic == i)
topic <- topic[order(-topic$gamma), ] %>% head()
}
for (i in c(1:10)) {
gettabletopicLDA(gamma.td, i) %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
) %>% print()
}
## Assignment of topic to term in each document
augment(lda) %>% head() %>%
kable() %>%
kable_styling(
bootstrap_options = c("striped", "hover", "condensed"),
full_width = FALSE
) %>% print()
install.packages("cowplot")
