% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/functions.R
\name{reddit_urls_mod}
\alias{reddit_urls_mod}
\title{Scrapping urls}
\usage{
reddit_urls_mod(
  search_terms = "",
  regex_filter = "",
  subreddit = NA,
  cn_threshold = 0,
  page_threshold = 1,
  sort_by = "relevance",
  time_frame = "week",
  wait_time = 2
)
}
\arguments{
\item{search_terms}{A \code{char} (character) used to specify what the user is looking for}

\item{regex_filter}{A \code{char} (character) used to specify what expression to filter}

\item{subreddit}{A \code{char} (character) used to specify what subreddit we want to scrape from. Default is NA}

\item{cn_threshold}{A \code{num} (numeric) used to specify the minimum number of commentaries in the discussion to scrap}

\item{sort_by}{A \code{char} (character) used to specify if we scrape by new, by relevance etc. Relevance is the default}

\item{time_frame}{A \code{char} (character) used to specify if we look only at last week, last month, or all}

\item{wait_time}{A \code{num} (numeric) used to specify the waiting time between scrappings.}

\item{page_treshold}{A \code{num} (numeric) used to specify the maxiumum number of pages to scrap}
}
\value{
A \code{dataframe} with the scraping done
}
\description{
Scrapping urls
}
\section{Functions}{
\itemize{
\item \code{reddit_urls_mod}: returns a dataframe scrapped using a url. It differs from the existing function with the addition of the time frame that is limited to one week.
}}

\examples{
Adresses<-reddit_urls_mod(search_terms = "federer", regex_filter = "", subreddit = "tennis",
cn_threshold = 1, page_threshold = 25, sort_by = "new", time_frame= "all", wait_time = 4)
}
